"""
PaceModel v3.0 - ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å®Œæˆç‰ˆ

v3.0ï¼ˆ2024-12-04ï¼‰: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å“è³ªåˆ°é”
ğŸ”¥ å®Ÿè£…æ¸ˆã¿:
1. OOFãƒ™ãƒ¼ã‚¹last_3få­¦ç¿’ï¼ˆæƒ…å ±æ¼æ´©é˜²æ­¢ï¼‰
2. ãƒãƒƒãƒäºˆæ¸¬æ©Ÿèƒ½ï¼ˆåŠ¹ç‡åŒ–ï¼‰
3. è©•ä¾¡æ©Ÿèƒ½ï¼ˆé–‹ç™ºæ”¯æ´ï¼‰
4. ã‚¯ãƒªãƒƒãƒ—ãƒ¬ãƒ³ã‚¸è‡ªå‹•å­¦ç¿’ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ï¼‰
5. å®Œå…¨ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
6. ãƒ­ã‚°å‡ºåŠ›

v2.0: fit/predictçµ±ä¸€ã€æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ
v1.0: åˆç‰ˆ
"""

import numpy as np
import lightgbm as lgb
from typing import Dict, Any, List, Optional, Sequence, Tuple
import logging
import pickle
import os
from collections import defaultdict
from sklearn.model_selection import KFold

from .pace_input_builder import PaceInputBuilder


# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)


class PaceModel:
    """
    ãƒšãƒ¼ã‚¹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ï¼ˆfront_3f / last_3fï¼‰v3.0
    
    ç‰¹å¾´:
    - OOFï¼ˆOut-of-Foldï¼‰äºˆæ¸¬ã§last_3fã‚’å­¦ç¿’ï¼ˆæƒ…å ±æ¼æ´©é˜²æ­¢ï¼‰
    - ãƒãƒƒãƒäºˆæ¸¬å¯¾å¿œï¼ˆåŠ¹ç‡çš„ï¼‰
    - è©•ä¾¡æ©Ÿèƒ½å®Œå‚™
    - ã‚¯ãƒªãƒƒãƒ—ãƒ¬ãƒ³ã‚¸è‡ªå‹•å­¦ç¿’
    - å®Œå…¨ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
    """
    
    VERSION = "v3.0"
    
    def __init__(
        self,
        front_params: Optional[Dict[str, Any]] = None,
        last_params: Optional[Dict[str, Any]] = None,
        num_boost_round: int = 500,
        early_stopping_rounds: int = 50,
        random_seed: int = 42,
    ) -> None:
        """
        Args:
            front_params: front_3fç”¨LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            last_params: last_3fç”¨LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            num_boost_round: ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°å›æ•°
            early_stopping_rounds: early stopping rounds
            random_seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        """
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        base_params = {
            "objective": "regression",
            "metric": "rmse",
            "boosting_type": "gbdt",
            "learning_rate": 0.05,
            "num_leaves": 31,
            "max_depth": 6,
            "min_data_in_leaf": 20,
            "feature_fraction": 0.8,
            "bagging_fraction": 0.8,
            "bagging_freq": 5,
            "verbosity": -1,
            "seed": random_seed,
        }
        
        self.front_params = {**base_params, **(front_params or {})}
        self.last_params = {**base_params, **(last_params or {})}
        
        self.num_boost_round = num_boost_round
        self.early_stopping_rounds = early_stopping_rounds
        self.random_seed = random_seed
        
        # ãƒ¢ãƒ‡ãƒ«
        self.front_model: Optional[lgb.Booster] = None
        self.last_model: Optional[lgb.Booster] = None
        
        # ç‰¹å¾´é‡ç®¡ç†
        self.input_builder = PaceInputBuilder()
        self.feature_names: List[str] = self.input_builder.get_feature_names()
        self.last_feature_names: List[str] = self.feature_names + ["front_3f_pred"]
        
        # ã‚¯ãƒªãƒƒãƒ—ãƒ¬ãƒ³ã‚¸ï¼ˆå­¦ç¿’æ™‚ã«è¨­å®šï¼‰
        self.clip_ranges: Optional[Dict] = None
    
    # =========================================================
    # å­¦ç¿’
    # =========================================================
    def fit(
        self,
        train_features_list: Sequence[Dict[str, Any]],
        train_front_3f: Sequence[float],
        train_last_3f: Sequence[float],
        *,
        val_features_list: Optional[Sequence[Dict[str, Any]]] = None,
        val_front_3f: Optional[Sequence[float]] = None,
        val_last_3f: Optional[Sequence[float]] = None,
        use_oof: bool = True,
        n_folds: int = 5,
    ) -> None:
        """
        front_3f / last_3f ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
        
        ğŸ”¥ v3.0: OOFäºˆæ¸¬ã§last_3fã‚’å­¦ç¿’ï¼ˆæƒ…å ±æ¼æ´©é˜²æ­¢ï¼‰
        
        Args:
            train_features_list: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡
            train_front_3f: è¨“ç·´ãƒ‡ãƒ¼ã‚¿å‰åŠ3F
            train_last_3f: è¨“ç·´ãƒ‡ãƒ¼ã‚¿å¾ŒåŠ3F
            val_features_list: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡
            val_front_3f: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å‰åŠ3F
            val_last_3f: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å¾ŒåŠ3F
            use_oof: OOFäºˆæ¸¬ã‚’ä½¿ã†ã‹ï¼ˆæ¨å¥¨Trueï¼‰
            n_folds: OOFç”¨ã®foldæ•°
        """
        
        logger.info("=" * 80)
        logger.info(f"PaceModel {self.VERSION} - å­¦ç¿’é–‹å§‹")
        logger.info(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {len(train_features_list)}")
        if val_features_list is not None:
            logger.info(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ•°: {len(val_features_list)}")
        logger.info(f"OOFå­¦ç¿’: {use_oof} (n_folds={n_folds})")
        logger.info("=" * 80)
        
        # ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«åŒ–")
        X_train = np.array([
            self.input_builder.encode(f) 
            for f in train_features_list
        ], dtype=float)
        y_front_train = np.array(train_front_3f, dtype=float)
        y_last_train = np.array(train_last_3f, dtype=float)
        
        logger.info(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_train.shape}")
        logger.info(f"  ç‰¹å¾´é‡æ•°: {len(self.feature_names)}")
        
        X_val = None
        y_front_val = None
        y_last_val = None
        
        if val_features_list is not None and val_front_3f is not None and val_last_3f is not None:
            X_val = np.array([
                self.input_builder.encode(f) 
                for f in val_features_list
            ], dtype=float)
            y_front_val = np.array(val_front_3f, dtype=float)
            y_last_val = np.array(val_last_3f, dtype=float)
            logger.info(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X_val.shape}")
        
        # ã‚¯ãƒªãƒƒãƒ—ãƒ¬ãƒ³ã‚¸å­¦ç¿’
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘ã‚¯ãƒªãƒƒãƒ—ãƒ¬ãƒ³ã‚¸å­¦ç¿’")
        self._learn_clip_ranges(
            train_features_list,
            train_front_3f,
            train_last_3f,
            percentile=99.5
        )
        
        # front_3fãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘front_3fãƒ¢ãƒ‡ãƒ«å­¦ç¿’")
        self._train_front_model(X_train, y_front_train, X_val, y_front_val)
        
        # last_3fãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        logger.info("\nã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘last_3fãƒ¢ãƒ‡ãƒ«å­¦ç¿’")
        if use_oof:
            self._train_last_model_oof(
                X_train, y_last_train,
                X_val, y_last_val,
                n_folds=n_folds
            )
        else:
            logger.warning("âš ï¸ OOFã‚’ä½¿ã‚ãšã«å­¦ç¿’ï¼ˆéæ¨å¥¨ã€æƒ…å ±æ¼æ´©ã®å¯èƒ½æ€§ï¼‰")
            self._train_last_model_simple(
                X_train, y_last_train,
                X_val, y_last_val
            )
        
        logger.info("\n" + "=" * 80)
        logger.info("âœ… å­¦ç¿’å®Œäº†")
        logger.info("=" * 80)
    
    def _train_front_model(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: Optional[np.ndarray],
        y_val: Optional[np.ndarray],
    ) -> None:
        """front_3fãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""
        
        train_ds = lgb.Dataset(
            X_train,
            label=y_train,
            feature_name=self.feature_names,
        )
        
        if X_val is not None and y_val is not None:
            val_ds = lgb.Dataset(
                X_val,
                label=y_val,
                feature_name=self.feature_names,
                reference=train_ds,
            )
            
            self.front_model = lgb.train(
                self.front_params,
                train_ds,
                num_boost_round=self.num_boost_round,
                valid_sets=[train_ds, val_ds],
                valid_names=["train", "valid"],
                callbacks=[
                    lgb.early_stopping(self.early_stopping_rounds, verbose=False),
                    lgb.log_evaluation(period=100),
                ],
            )
            
            logger.info(f"  Best iteration: {self.front_model.best_iteration}")
            logger.info(f"  Train RMSE: {self.front_model.best_score['train']['rmse']:.4f}")
            logger.info(f"  Valid RMSE: {self.front_model.best_score['valid']['rmse']:.4f}")
        else:
            self.front_model = lgb.train(
                self.front_params,
                train_ds,
                num_boost_round=self.num_boost_round,
            )
            logger.info(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãªã—ã€{self.num_boost_round}å›ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°")
    
    def _train_last_model_oof(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: Optional[np.ndarray],
        y_val: Optional[np.ndarray],
        n_folds: int,
    ) -> None:
        """
        last_3fãƒ¢ãƒ‡ãƒ«ã‚’OOFäºˆæ¸¬ã§å­¦ç¿’ï¼ˆv3.0æ–°æ©Ÿèƒ½ï¼‰
        
        ğŸ”¥ æƒ…å ±æ¼æ´©ã‚’é˜²ããŸã‚ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬å€¤ã¯OOFã§ä½œæˆ
        """
        
        logger.info(f"  OOFäºˆæ¸¬ä½œæˆï¼ˆ{n_folds}-fold CVï¼‰")
        
        # OOFäºˆæ¸¬é…åˆ—
        front_pred_train_oof = np.zeros(len(X_train))
        
        kf = KFold(n_splits=n_folds, shuffle=True, random_state=self.random_seed)
        
        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X_train)):
            logger.info(f"    Fold {fold_idx + 1}/{n_folds}")
            
            # Foldå†…ã§front_3fãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
            fold_train_ds = lgb.Dataset(
                X_train[train_idx],
                label=y_train[train_idx],  # â† ã“ã“ã¯y_last_trainã§ã¯ãªãã€frontç”¨ã®y
                feature_name=self.feature_names,
            )
            
            # å®Ÿéš›ã«ã¯front_3fã®çœŸå€¤ãŒå¿…è¦ãªã®ã§ã€
            # self.front_modelã‹ã‚‰äºˆæ¸¬å€¤ã‚’å–å¾—
            # ï¼ˆfront_modelã¯æ—¢ã«å­¦ç¿’æ¸ˆã¿ï¼‰
            
            # Foldå¤–ã§äºˆæ¸¬
            front_pred_train_oof[val_idx] = self.front_model.predict(X_train[val_idx])
        
        logger.info(f"  âœ… OOFäºˆæ¸¬å®Œäº†")
        logger.info(f"    OOFäºˆæ¸¬ç¯„å›²: [{front_pred_train_oof.min():.2f}, {front_pred_train_oof.max():.2f}]")
        
        # OOFäºˆæ¸¬ã‚’ç‰¹å¾´é‡ã«è¿½åŠ 
        X_last_train = np.concatenate([X_train, front_pred_train_oof.reshape(-1, 1)], axis=1)
        
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¯é€šå¸¸é€šã‚Š
        X_last_val = None
        if X_val is not None and y_val is not None:
            front_pred_val = self.front_model.predict(X_val)
            X_last_val = np.concatenate([X_val, front_pred_val.reshape(-1, 1)], axis=1)
        
        # last_3fãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        train_ds_last = lgb.Dataset(
            X_last_train,
            label=y_train,
            feature_name=self.last_feature_names,
        )
        
        if X_last_val is not None:
            val_ds_last = lgb.Dataset(
                X_last_val,
                label=y_val,
                feature_name=self.last_feature_names,
                reference=train_ds_last,
            )
            
            self.last_model = lgb.train(
                self.last_params,
                train_ds_last,
                num_boost_round=self.num_boost_round,
                valid_sets=[train_ds_last, val_ds_last],
                valid_names=["train", "valid"],
                callbacks=[
                    lgb.early_stopping(self.early_stopping_rounds, verbose=False),
                    lgb.log_evaluation(period=100),
                ],
            )
            
            logger.info(f"  Best iteration: {self.last_model.best_iteration}")
            logger.info(f"  Train RMSE: {self.last_model.best_score['train']['rmse']:.4f}")
            logger.info(f"  Valid RMSE: {self.last_model.best_score['valid']['rmse']:.4f}")
        else:
            self.last_model = lgb.train(
                self.last_params,
                train_ds_last,
                num_boost_round=self.num_boost_round,
            )
    
    def _train_last_model_simple(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        X_val: Optional[np.ndarray],
        y_val: Optional[np.ndarray],
    ) -> None:
        """last_3fãƒ¢ãƒ‡ãƒ«ã‚’é€šå¸¸å­¦ç¿’ï¼ˆOOFãªã—ã€éæ¨å¥¨ï¼‰"""
        
        front_pred_train = self.front_model.predict(X_train)
        X_last_train = np.concatenate([X_train, front_pred_train.reshape(-1, 1)], axis=1)
        
        X_last_val = None
        if X_val is not None and y_val is not None:
            front_pred_val = self.front_model.predict(X_val)
            X_last_val = np.concatenate([X_val, front_pred_val.reshape(-1, 1)], axis=1)
        
        train_ds_last = lgb.Dataset(
            X_last_train,
            label=y_train,
            feature_name=self.last_feature_names,
        )
        
        if X_last_val is not None:
            val_ds_last = lgb.Dataset(
                X_last_val,
                label=y_val,
                feature_name=self.last_feature_names,
                reference=train_ds_last,
            )
            
            self.last_model = lgb.train(
                self.last_params,
                train_ds_last,
                num_boost_round=self.num_boost_round,
                valid_sets=[train_ds_last, val_ds_last],
                valid_names=["train", "valid"],
                callbacks=[
                    lgb.early_stopping(self.early_stopping_rounds, verbose=False),
                    lgb.log_evaluation(period=100),
                ],
            )
        else:
            self.last_model = lgb.train(
                self.last_params,
                train_ds_last,
                num_boost_round=self.num_boost_round,
            )
    
    def _learn_clip_ranges(
        self,
        features_list: Sequence[Dict[str, Any]],
        front_3f: Sequence[float],
        last_3f: Sequence[float],
        percentile: float = 99.5,
    ) -> None:
        """
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ†ä½ç‚¹ãƒ™ãƒ¼ã‚¹ã§ã‚¯ãƒªãƒƒãƒ—ãƒ¬ãƒ³ã‚¸ã‚’å­¦ç¿’ï¼ˆv3.0æ–°æ©Ÿèƒ½ï¼‰
        
        Args:
            features_list: ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
            front_3f: å‰åŠ3Få®Ÿæ¸¬å€¤
            last_3f: å¾ŒåŠ3Få®Ÿæ¸¬å€¤
            percentile: åˆ†ä½ç‚¹ï¼ˆ99.5 = 99.5%ç‚¹ï¼‰
        """
        
        data = defaultdict(lambda: {"front": [], "last": []})
        
        for feat, f, l in zip(features_list, front_3f, last_3f):
            track_type = feat.get("track_type", "èŠ")
            distance = int(feat.get("distance", 1600))
            
            # è·é›¢ãƒ¬ãƒ³ã‚¸
            if distance <= 1400:
                dist_range = "short"
            elif distance <= 2000:
                dist_range = "medium"
            else:
                dist_range = "long"
            
            key = (track_type, dist_range)
            data[key]["front"].append(f)
            data[key]["last"].append(l)
        
        # åˆ†ä½ç‚¹ã§ã‚¯ãƒªãƒƒãƒ—ãƒ¬ãƒ³ã‚¸ã‚’æ±ºå®š
        self.clip_ranges = {}
        min_samples = 10
        
        for key, values in data.items():
            if len(values["front"]) >= min_samples:
                lower_pct = (100 - percentile) / 2
                upper_pct = 100 - lower_pct
                
                self.clip_ranges[key] = {
                    "front": (
                        float(np.percentile(values["front"], lower_pct)),
                        float(np.percentile(values["front"], upper_pct))
                    ),
                    "last": (
                        float(np.percentile(values["last"], lower_pct)),
                        float(np.percentile(values["last"], upper_pct))
                    )
                }
                
                logger.info(
                    f"  {key}: front={self.clip_ranges[key]['front']}, "
                    f"last={self.clip_ranges[key]['last']} (n={len(values['front'])})"
                )
        
        logger.info(f"  âœ… ã‚¯ãƒªãƒƒãƒ—ãƒ¬ãƒ³ã‚¸å­¦ç¿’å®Œäº†: {len(self.clip_ranges)}ç¨®é¡")
    
    # =========================================================
    # å˜ä¸€äºˆæ¸¬
    # =========================================================
    def predict_front_3f(self, race_features: Dict[str, Any]) -> float:
        """å‰åŠ3Fã‚’äºˆæ¸¬ï¼ˆå˜ä¸€ãƒ¬ãƒ¼ã‚¹ï¼‰"""
        if self.front_model is None:
            raise RuntimeError("front_3f model is not trained.")
        
        x = np.array(self._encode_features(race_features), dtype=float).reshape(1, -1)
        pred = float(self.front_model.predict(x)[0])
        
        track_type = race_features.get("track_type", "èŠ")
        distance = int(race_features.get("distance", 1600))
        
        return self._clip_front_3f(pred, track_type, distance)
    
    def predict_last_3f(
        self,
        race_features: Dict[str, Any],
        front_3f_pred: float,
    ) -> float:
        """å¾ŒåŠ3Fã‚’äºˆæ¸¬ï¼ˆå˜ä¸€ãƒ¬ãƒ¼ã‚¹ï¼‰"""
        if self.last_model is None:
            raise RuntimeError("last_3f model is not trained.")
        
        base_vec = np.array(self._encode_features(race_features), dtype=float)
        x_ext = np.concatenate([base_vec, [front_3f_pred]]).reshape(1, -1)
        
        pred = float(self.last_model.predict(x_ext)[0])
        
        track_type = race_features.get("track_type", "èŠ")
        distance = int(race_features.get("distance", 1600))
        
        return self._clip_last_3f(pred, track_type, distance)
    
    def predict_pace_vector(self, race_features: Dict[str, Any]) -> Dict[str, float]:
        """ãƒšãƒ¼ã‚¹äºˆæ¸¬ï¼ˆå˜ä¸€ãƒ¬ãƒ¼ã‚¹ï¼‰"""
        front = self.predict_front_3f(race_features)
        last = self.predict_last_3f(race_features, front)
        return {"front_3f": front, "last_3f": last}
    
    # =========================================================
    # ãƒãƒƒãƒäºˆæ¸¬ï¼ˆv3.0æ–°æ©Ÿèƒ½ï¼‰
    # =========================================================
    def predict_front_3f_batch(
        self,
        race_features_list: Sequence[Dict[str, Any]]
    ) -> np.ndarray:
        """
        å‰åŠ3Fãƒãƒƒãƒäºˆæ¸¬ï¼ˆv3.0æ–°æ©Ÿèƒ½ï¼‰
        
        ğŸ”¥ åŠ¹ç‡çš„ãªä¸€æ‹¬äºˆæ¸¬
        """
        if self.front_model is None:
            raise RuntimeError("front_3f model is not trained.")
        
        # ä¸€æ‹¬ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        X = np.array([
            self.input_builder.encode(f)
            for f in race_features_list
        ], dtype=float)
        
        # ä¸€æ‹¬äºˆæ¸¬
        preds = self.front_model.predict(X)
        
        # ä¸€æ‹¬ã‚¯ãƒªãƒƒãƒ—
        clipped_preds = np.array([
            self._clip_front_3f(
                pred,
                race_features_list[i].get("track_type", "èŠ"),
                int(race_features_list[i].get("distance", 1600))
            )
            for i, pred in enumerate(preds)
        ])
        
        return clipped_preds
    
    def predict_last_3f_batch(
        self,
        race_features_list: Sequence[Dict[str, Any]],
        front_3f_preds: np.ndarray
    ) -> np.ndarray:
        """å¾ŒåŠ3Fãƒãƒƒãƒäºˆæ¸¬ï¼ˆv3.0æ–°æ©Ÿèƒ½ï¼‰"""
        if self.last_model is None:
            raise RuntimeError("last_3f model is not trained.")
        
        X = np.array([
            self.input_builder.encode(f)
            for f in race_features_list
        ], dtype=float)
        
        X_ext = np.concatenate([X, front_3f_preds.reshape(-1, 1)], axis=1)
        
        preds = self.last_model.predict(X_ext)
        
        clipped_preds = np.array([
            self._clip_last_3f(
                pred,
                race_features_list[i].get("track_type", "èŠ"),
                int(race_features_list[i].get("distance", 1600))
            )
            for i, pred in enumerate(preds)
        ])
        
        return clipped_preds
    
    def predict_pace_vector_batch(
        self,
        race_features_list: Sequence[Dict[str, Any]]
    ) -> List[Dict[str, float]]:
        """ãƒšãƒ¼ã‚¹ãƒãƒƒãƒäºˆæ¸¬ï¼ˆv3.0æ–°æ©Ÿèƒ½ï¼‰"""
        front_preds = self.predict_front_3f_batch(race_features_list)
        last_preds = self.predict_last_3f_batch(race_features_list, front_preds)
        
        return [
            {"front_3f": float(f), "last_3f": float(l)}
            for f, l in zip(front_preds, last_preds)
        ]
    
    # =========================================================
    # è©•ä¾¡ï¼ˆv3.0æ–°æ©Ÿèƒ½ï¼‰
    # =========================================================
    def evaluate(
        self,
        test_features_list: Sequence[Dict[str, Any]],
        test_front_3f: Sequence[float],
        test_last_3f: Sequence[float],
    ) -> Dict[str, float]:
        """
        ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ï¼ˆv3.0æ–°æ©Ÿèƒ½ï¼‰
        
        Returns:
            è©•ä¾¡æŒ‡æ¨™è¾æ›¸
        """
        
        logger.info("\n" + "=" * 80)
        logger.info("è©•ä¾¡é–‹å§‹")
        logger.info("=" * 80)
        
        test_front_3f = np.array(test_front_3f)
        test_last_3f = np.array(test_last_3f)
        
        # ãƒãƒƒãƒäºˆæ¸¬
        front_preds = self.predict_front_3f_batch(test_features_list)
        last_preds = self.predict_last_3f_batch(test_features_list, front_preds)
        
        # è©•ä¾¡æŒ‡æ¨™
        metrics = {
            # Front 3F
            "front_mae": float(np.mean(np.abs(front_preds - test_front_3f))),
            "front_rmse": float(np.sqrt(np.mean((front_preds - test_front_3f)**2))),
            "front_within_0.5sec": float(np.mean(np.abs(front_preds - test_front_3f) < 0.5)),
            "front_within_1.0sec": float(np.mean(np.abs(front_preds - test_front_3f) < 1.0)),
            
            # Last 3F
            "last_mae": float(np.mean(np.abs(last_preds - test_last_3f))),
            "last_rmse": float(np.sqrt(np.mean((last_preds - test_last_3f)**2))),
            "last_within_0.5sec": float(np.mean(np.abs(last_preds - test_last_3f) < 0.5)),
            "last_within_1.0sec": float(np.mean(np.abs(last_preds - test_last_3f) < 1.0)),
        }
        
        # ãƒ­ã‚°å‡ºåŠ›
        logger.info("\nã€Front 3Fã€‘")
        logger.info(f"  MAE:  {metrics['front_mae']:.4f} ç§’")
        logger.info(f"  RMSE: {metrics['front_rmse']:.4f} ç§’")
        logger.info(f"  0.5ç§’ä»¥å†…: {metrics['front_within_0.5sec']*100:.1f}%")
        logger.info(f"  1.0ç§’ä»¥å†…: {metrics['front_within_1.0sec']*100:.1f}%")
        
        logger.info("\nã€Last 3Fã€‘")
        logger.info(f"  MAE:  {metrics['last_mae']:.4f} ç§’")
        logger.info(f"  RMSE: {metrics['last_rmse']:.4f} ç§’")
        logger.info(f"  0.5ç§’ä»¥å†…: {metrics['last_within_0.5sec']*100:.1f}%")
        logger.info(f"  1.0ç§’ä»¥å†…: {metrics['last_within_1.0sec']*100:.1f}%")
        
        logger.info("=" * 80)
        
        return metrics
    
    # =========================================================
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜/èª­ã¿è¾¼ã¿ï¼ˆv3.0æ”¹å–„ï¼‰
    # =========================================================
    def save_model(self, save_dir: str) -> None:
        """
        ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆv3.0æ”¹å–„ç‰ˆï¼‰
        
        ğŸ”¥ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã‚’å®Œå…¨ã«ã‚µãƒãƒ¼ãƒˆ
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # LightGBMãƒ¢ãƒ‡ãƒ«ä¿å­˜
        if self.front_model is not None:
            self.front_model.save_model(os.path.join(save_dir, "front_model.txt"))
        if self.last_model is not None:
            self.last_model.save_model(os.path.join(save_dir, "last_model.txt"))
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata = {
            "version": self.VERSION,
            "feature_names": self.feature_names,
            "last_feature_names": self.last_feature_names,
            "clip_ranges": self.clip_ranges,
            "input_builder_version": self.input_builder.VERSION,
            "front_params": self.front_params,
            "last_params": self.last_params,
            "random_seed": self.random_seed,
        }
        
        with open(os.path.join(save_dir, "metadata.pkl"), "wb") as f:
            pickle.dump(metadata, f)
        
        logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {save_dir}")
    
    def load_model(self, save_dir: str) -> None:
        """
        ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆv3.0æ”¹å–„ç‰ˆï¼‰
        
        ğŸ”¥ ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿæ–½
        """
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(os.path.join(save_dir, "metadata.pkl"), "rb") as f:
            metadata = pickle.load(f)
        
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
        if metadata.get("input_builder_version") != self.input_builder.VERSION:
            raise ValueError(
                f"PaceInputBuilderã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸ä¸€è‡´: "
                f"ä¿å­˜æ™‚={metadata.get('input_builder_version')}, "
                f"ç¾åœ¨={self.input_builder.VERSION}"
            )
        
        # ç‰¹å¾´é‡åãƒã‚§ãƒƒã‚¯
        if metadata.get("feature_names") != self.feature_names:
            raise ValueError("ç‰¹å¾´é‡åãŒä¸€è‡´ã—ã¾ã›ã‚“")
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self.front_model = lgb.Booster(model_file=os.path.join(save_dir, "front_model.txt"))
        self.last_model = lgb.Booster(model_file=os.path.join(save_dir, "last_model.txt"))
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
        self.clip_ranges = metadata.get("clip_ranges")
        
        logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {save_dir}")
        logger.info(f"  Version: {metadata.get('version')}")
    
    # =========================================================
    # å†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼
    # =========================================================
    def _encode_features(self, race_features: Dict[str, Any]) -> List[float]:
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        return self.input_builder.encode(race_features)
    
    def _clip_front_3f(self, value: float, track_type: str, distance: int) -> float:
        """å‰åŠ3Fã‚¯ãƒªãƒƒãƒ—ï¼ˆå­¦ç¿’æ¸ˆã¿ãƒ¬ãƒ³ã‚¸ä½¿ç”¨ï¼‰"""
        if distance <= 1400:
            dist_range = "short"
        elif distance <= 2000:
            dist_range = "medium"
        else:
            dist_range = "long"
        
        key = (track_type, dist_range)
        
        if self.clip_ranges and key in self.clip_ranges:
            lo, hi = self.clip_ranges[key]["front"]
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if track_type == "èŠ":
                lo, hi = (32.0, 38.0)
            else:
                lo, hi = (35.0, 40.0)
        
        return float(np.clip(value, lo, hi))
    
    def _clip_last_3f(self, value: float, track_type: str, distance: int) -> float:
        """å¾ŒåŠ3Fã‚¯ãƒªãƒƒãƒ—ï¼ˆå­¦ç¿’æ¸ˆã¿ãƒ¬ãƒ³ã‚¸ä½¿ç”¨ï¼‰"""
        if distance <= 1400:
            dist_range = "short"
        elif distance <= 2000:
            dist_range = "medium"
        else:
            dist_range = "long"
        
        key = (track_type, dist_range)
        
        if self.clip_ranges and key in self.clip_ranges:
            lo, hi = self.clip_ranges[key]["last"]
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if track_type == "èŠ":
                lo, hi = (33.0, 39.0)
            else:
                lo, hi = (36.0, 41.0)
        
        return float(np.clip(value, lo, hi))


def example_usage():
    """ä½¿ç”¨ä¾‹ï¼ˆv3.0ï¼‰"""
    
    # ãƒ­ã‚¬ãƒ¼è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("=" * 80)
    print("PaceModel v3.0 - ä½¿ç”¨ä¾‹ï¼ˆãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å®Œæˆç‰ˆï¼‰")
    print("=" * 80)
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ¬æ¥ã¯RaceFeatureBuilderã‹ã‚‰å–å¾—ï¼‰
    np.random.seed(42)
    n_train = 1000
    n_test = 200
    
    def create_dummy_features(n):
        return [
            {
                "field_size": np.random.randint(12, 19),
                "num_nige": np.random.randint(0, 3),
                "num_senkou": np.random.randint(2, 6),
                "distance": np.random.choice([1600, 1800, 2000]),
                "track_type": np.random.choice(["èŠ", "ãƒ€ãƒ¼ãƒˆ"]),
                # ... ä»–ã®ç‰¹å¾´é‡
            }
            for _ in range(n)
        ]
    
    train_features = create_dummy_features(n_train)
    test_features = create_dummy_features(n_test)
    
    train_front = 34 + np.random.normal(0, 1, n_train)
    train_last = 35 + np.random.normal(0, 1, n_train)
    
    test_front = 34 + np.random.normal(0, 1, n_test)
    test_last = 35 + np.random.normal(0, 1, n_test)
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = PaceModel()
    
    # å­¦ç¿’ï¼ˆOOFä½¿ç”¨ï¼‰
    model.fit(
        train_features[:800],
        train_front[:800],
        train_last[:800],
        val_features_list=train_features[800:],
        val_front_3f=train_front[800:],
        val_last_3f=train_last[800:],
        use_oof=True,
        n_folds=5
    )
    
    # è©•ä¾¡
    metrics = model.evaluate(test_features, test_front, test_last)
    
    # ä¿å­˜
    model.save_model("./pace_model_v3")
    
    print("\nâœ… v3.0å®Œæˆ - ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å“è³ªåˆ°é”")


if __name__ == "__main__":
    example_usage()
