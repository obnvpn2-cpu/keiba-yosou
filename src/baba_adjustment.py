"""
é¦¬å ´è£œæ­£ãƒ¢ãƒ‡ãƒ«ï¼ˆv2.0 - ChatGPTç‰ˆãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾å¿œå®Œå…¨ç‰ˆï¼‰

v2.0ï¼ˆ2024-12-04ï¼‰: ChatGPTç‰ˆãƒ¬ãƒ“ãƒ¥ãƒ¼ + Claudeè¿½åŠ ä¿®æ­£
ğŸ”¥ å®Ÿé‹ç”¨ãƒ¬ãƒ™ãƒ«å®Œæˆ:
1. race_dateå‹å¤‰æ›ã‚’æ˜ç¤ºçš„ã«ï¼ˆtry-exceptã‹ã‚‰pd.to_datetimeã¸ï¼‰
2. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’configåŒ–ï¼ˆprior_win_rateï¼‰
3. track_statisticsç•°å¸¸æ™‚ã«è­¦å‘Šè¿½åŠ 
4. targetã‚¯ãƒªãƒƒãƒ—ã‚’åˆ†ä½ç‚¹ãƒ™ãƒ¼ã‚¹ã«å¤‰æ›´
5. å…¨ä½“çš„ãªã‚³ãƒ¼ãƒ‰å“è³ªå‘ä¸Š

v1.0: åˆç‰ˆï¼ˆChatGPTä¿®æ­£ç‰ˆï¼‰
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
from scipy.special import logit, expit
from typing import Dict, Optional, Tuple
import warnings


class BabaAdjustmentModel:
    """
    é¦¬å ´è£œæ­£ãƒ¢ãƒ‡ãƒ«ï¼ˆlog-oddsç©ºé–“ã§ã®è£œæ­£ï¼‰

    ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
    1. ãƒ™ãƒ¼ã‚¹äºˆæ¸¬ã¨ã® log-odds å·® (delta) ã‚’å­¦ç¿’
    2. å¤–ã‚Œå€¤ã‚’åˆ†ä½ç‚¹ãƒ™ãƒ¼ã‚¹ã§ã‚¯ãƒªãƒƒãƒ—ğŸ”¥
    3. é¦¬ã”ã¨ã®å¯¾è±¡é¦¬å ´ãƒ¬ãƒ¼ã‚¹æ•°ã«å¿œã˜ã¦ Shrinkage
    """

    def __init__(
        self,
        alpha: float = 0.5,
        clip_percentile: float = 99.0,
        min_data_for_full_weight: int = 10,
        params: Optional[Dict] = None
    ):
        """
        Args:
            alpha: ãƒ©ãƒ—ãƒ©ã‚¹ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ä¿‚æ•°
            clip_percentile: targetã‚’ã“ã®åˆ†ä½ç‚¹ã§ã‚¯ãƒªãƒƒãƒ—ï¼ˆ99.0=99%ç‚¹ï¼‰ğŸ”¥
            min_data_for_full_weight: Shrinkageã§å®Œå…¨ä¿¡é ¼ã¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ•°
            params: LightGBMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        self.alpha = alpha
        self.clip_percentile = clip_percentile
        self.min_data_for_full_weight = min_data_for_full_weight

        if params is None:
            params = {
                "objective": "regression",
                "metric": "rmse",
                "boosting_type": "gbdt",
                "learning_rate": 0.05,
                "num_leaves": 15,
                "max_depth": 5,
                "min_child_samples": 20,
                "subsample": 0.8,
                "colsample_bytree": 0.8,
                "reg_alpha": 0.1,
                "reg_lambda": 0.1,
                "verbose": -1,
            }

        self.params = params
        self.model: Optional[lgb.Booster] = None
        self.feature_names = None
        self.target_lower: Optional[float] = None
        self.target_upper: Optional[float] = None

    def prepare_training_data(
        self,
        calibrated_pred: np.ndarray,
        actual_win: np.ndarray,
        features: pd.DataFrame,
        horse_baba_race_counts: Optional[np.ndarray] = None,
    ) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:
        """
        å­¦ç¿’ç”¨ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã‚µãƒ³ãƒ—ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã‚’æº–å‚™ï¼ˆv2.0æ”¹å–„ç‰ˆï¼‰

        Args:
            calibrated_pred: ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¸ˆã¿äºˆæ¸¬å‹ç‡
            actual_win: å®Ÿéš›ã®å‹åˆ©ï¼ˆ1/0ï¼‰
            features: é¦¬å ´é–¢é€£ã®ç‰¹å¾´é‡
            horse_baba_race_counts: å„é¦¬ã®å¯¾è±¡é¦¬å ´æ¡ä»¶ã§ã®ãƒ¬ãƒ¼ã‚¹æ•°

        Returns:
            (ç‰¹å¾´é‡, target_clipped, sample_weight)
        """
        # smoothing: smoothed_actual = (win + Î±) / (1 + 2Î±)
        smoothed_actual = (actual_win + self.alpha) / (1.0 + 2.0 * self.alpha)

        # logitè¨ˆç®—ï¼ˆ0,1ç›´æ’ƒã‚’é¿ã‘ã‚‹ï¼‰
        calibrated_pred_clipped = np.clip(calibrated_pred, 0.001, 0.999)
        smoothed_actual_clipped = np.clip(smoothed_actual, 0.001, 0.999)

        logit_pred = logit(calibrated_pred_clipped)
        logit_actual = logit(smoothed_actual_clipped)

        # ç›®çš„å¤‰æ•° = logit(actual) - logit(pred)
        target = logit_actual - logit_pred

        # ğŸ”¥ v2.0: åˆ†ä½ç‚¹ãƒ™ãƒ¼ã‚¹ã§ã‚¯ãƒªãƒƒãƒ—ï¼ˆClaudeæŒ‡æ‘˜ï¼‰
        lower_percentile = (100 - self.clip_percentile) / 2
        upper_percentile = 100 - lower_percentile
        
        self.target_lower = float(np.percentile(target, lower_percentile))
        self.target_upper = float(np.percentile(target, upper_percentile))

        target_clipped = np.clip(target, self.target_lower, self.target_upper)

        # ã‚µãƒ³ãƒ—ãƒ«ã‚¦ã‚§ã‚¤ãƒˆï¼ˆãƒ‡ãƒ¼ã‚¿é‡ã«å¿œã˜ãŸä¿¡é ¼åº¦ï¼‰
        if horse_baba_race_counts is not None:
            if len(horse_baba_race_counts) != len(target_clipped):
                raise ValueError(
                    "horse_baba_race_counts ã®é•·ã•ãŒç‰¹å¾´é‡ã¨ä¸€è‡´ã—ã¦ã„ã¾ã›ã‚“"
                )
            sample_weight = np.minimum(
                1.0,
                horse_baba_race_counts / float(self.min_data_for_full_weight),
            )
        else:
            sample_weight = np.ones(len(target_clipped), dtype=float)

        return features, target_clipped, sample_weight

    def train(
        self,
        X: pd.DataFrame,
        y: np.ndarray,
        sample_weight: Optional[np.ndarray] = None,
        X_val: Optional[pd.DataFrame] = None,
        y_val: Optional[np.ndarray] = None,
        sample_weight_val: Optional[np.ndarray] = None,
    ):
        """ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        self.feature_names = X.columns.tolist()

        train_data = lgb.Dataset(
            X,
            label=y,
            weight=sample_weight,
            free_raw_data=False,
        )

        valid_sets = [train_data]
        valid_names = ["train"]

        if X_val is not None and y_val is not None:
            val_data = lgb.Dataset(
                X_val,
                label=y_val,
                weight=sample_weight_val,
                reference=train_data,
                free_raw_data=False,
            )
            valid_sets.append(val_data)
            valid_names.append("valid")

        self.model = lgb.train(
            self.params,
            train_data,
            num_boost_round=500,
            valid_sets=valid_sets,
            valid_names=valid_names,
            callbacks=[
                lgb.early_stopping(stopping_rounds=50, verbose=False),
            ],
        )

        print(f"âœ… BabaAdjustmentModel è¨“ç·´å®Œäº† - best_iteration: {self.model.best_iteration}")

    def predict_delta(
        self,
        features: pd.DataFrame,
        horse_baba_race_counts: Optional[np.ndarray] = None,
        apply_shrinkage: bool = True,
    ) -> np.ndarray:
        """é¦¬å ´è£œæ­£é‡ï¼ˆlog-oddså·®ï¼‰ã‚’äºˆæ¸¬"""
        if self.model is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        delta = self.model.predict(features, num_iteration=self.model.best_iteration)

        # Shrinkageé©ç”¨
        if apply_shrinkage and horse_baba_race_counts is not None:
            if len(horse_baba_race_counts) != len(delta):
                raise ValueError(
                    "horse_baba_race_counts ã®é•·ã•ãŒäºˆæ¸¬å€¤ã¨ä¸€è‡´ã—ã¦ã„ã¾ã›ã‚“"
                )
            weight = np.minimum(
                1.0,
                horse_baba_race_counts / float(self.min_data_for_full_weight),
            )
            delta = weight * delta

        return delta

    def apply_adjustment(
        self,
        calibrated_pred: np.ndarray,
        delta_baba: np.ndarray,
    ) -> np.ndarray:
        """ãƒ™ãƒ¼ã‚¹äºˆæ¸¬ã«é¦¬å ´è£œæ­£ã‚’é©ç”¨ã—ã¦æœ€çµ‚å‹ç‡ã«å¤‰æ›"""
        calibrated_pred_clipped = np.clip(calibrated_pred, 0.001, 0.999)
        logit_base = logit(calibrated_pred_clipped)

        logit_final = logit_base + delta_baba
        final_prob = expit(logit_final)

        return final_prob

    def get_feature_importance(
        self,
        importance_type: str = "gain",
        top_n: int = 10,
    ) -> pd.DataFrame:
        """ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—"""
        if self.model is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        importance = self.model.feature_importance(importance_type=importance_type)
        df = (
            pd.DataFrame({"feature": self.feature_names, "importance": importance})
            .sort_values("importance", ascending=False)
            .head(top_n)
        )
        return df


class BabaFeatureExtractor:
    """
    é¦¬å ´è£œæ­£ç”¨ã®ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹ï¼ˆv2.0æ”¹å–„ç‰ˆï¼‰

    ğŸ”¥ v2.0ã§ã®æ”¹å–„:
    - race_dateå‹å¤‰æ›ã‚’æ˜ç¤ºçš„ã«
    - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’configåŒ–
    - track_statisticsç•°å¸¸æ™‚ã«è­¦å‘Š
    """

    def __init__(
        self, 
        track_statistics: Dict, 
        date_column: str = "race_date",
        prior_win_rate: float = 0.1
    ):
        """
        Args:
            track_statistics: ç«¶é¦¬å ´åˆ¥ã®çµ±è¨ˆ
            date_column: ãƒ¬ãƒ¼ã‚¹æ—¥ä»˜ã‚«ãƒ©ãƒ å
            prior_win_rate: ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹ç‡ğŸ”¥
        """
        self.track_statistics = track_statistics
        self.date_column = date_column
        self.prior_win_rate = prior_win_rate

    def extract_features(
        self,
        race_data: pd.DataFrame,
        horse_history: pd.DataFrame,
    ) -> pd.DataFrame:
        """
        é¦¬å ´è£œæ­£ç”¨ã®ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆv2.0æ”¹å–„ç‰ˆï¼‰

        Args:
            race_data: äºˆæ¸¬å¯¾è±¡ãƒ¬ãƒ¼ã‚¹ã®æƒ…å ±
            horse_history: å„é¦¬ã®éå»æˆç¸¾

        Returns:
            ç‰¹å¾´é‡DataFrame
        """
        features = []

        for _, race in race_data.iterrows():
            track = race["track_name"]

            # ç«¶é¦¬å ´åˆ¥ã®æ¨™æº–åŒ–ç‰¹å¾´é‡
            normalized_cushion = 0.0
            normalized_moisture = 0.0

            if track in self.track_statistics:
                stats = self.track_statistics[track]

                # ğŸ”¥ v2.0: track_statisticsç•°å¸¸æ™‚ã«è­¦å‘Š
                if (
                    "cushion_value" in race
                    and "avg_cushion" in stats
                    and "std_cushion" in stats
                ):
                    if stats["std_cushion"] in (0, None):
                        warnings.warn(
                            f"ç«¶é¦¬å ´ '{track}' ã®std_cushionãŒ0ã¾ãŸã¯Noneã§ã™ã€‚"
                            "normalized_cushionã¯0ã«ã‚»ãƒƒãƒˆã•ã‚Œã¾ã™ã€‚",
                            UserWarning
                        )
                    else:
                        normalized_cushion = (
                            race["cushion_value"] - stats["avg_cushion"]
                        ) / stats["std_cushion"]

                if (
                    "moisture" in race
                    and "avg_moisture" in stats
                    and "std_moisture" in stats
                ):
                    if stats["std_moisture"] in (0, None):
                        warnings.warn(
                            f"ç«¶é¦¬å ´ '{track}' ã®std_moistureãŒ0ã¾ãŸã¯Noneã§ã™ã€‚"
                            "normalized_moistureã¯0ã«ã‚»ãƒƒãƒˆã•ã‚Œã¾ã™ã€‚",
                            UserWarning
                        )
                    else:
                        normalized_moisture = (
                            race["moisture"] - stats["avg_moisture"]
                        ) / stats["std_moisture"]

            # é¦¬ã”ã¨ã®é¦¬å ´é©æ€§çš„ãªç‰¹å¾´é‡
            horse_id = race["horse_id"]

            # ğŸ”¥ v2.0: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’configåŒ–ï¼ˆClaudeæŒ‡æ‘˜ï¼‰
            high_speed_win_rate = self.prior_win_rate
            slow_win_rate = self.prior_win_rate
            high_speed_count = 0
            slow_count = 0

            if "horse_id" in horse_history.columns:
                horse_past = horse_history[horse_history["horse_id"] == horse_id]

                # ğŸ”¥ v2.0: race_dateå‹å¤‰æ›ã‚’æ˜ç¤ºçš„ã«ï¼ˆClaudeæŒ‡æ‘˜ï¼‰
                race_date = None
                if self.date_column in race.index:
                    race_date = race[self.date_column]
                elif self.date_column in race_data.columns:
                    race_date = race[self.date_column]

                if race_date is not None and self.date_column in horse_past.columns:
                    # æ˜ç¤ºçš„ã«å‹å¤‰æ›
                    if not pd.api.types.is_datetime64_any_dtype(horse_past[self.date_column]):
                        horse_past = horse_past.copy()
                        horse_past[self.date_column] = pd.to_datetime(
                            horse_past[self.date_column], errors='coerce'
                        )
                    
                    if not pd.api.types.is_datetime64_any_dtype(pd.Series([race_date])):
                        race_date = pd.to_datetime(race_date, errors='coerce')
                    
                    # æ™‚ç³»åˆ—ãƒªãƒ¼ã‚¯é˜²æ­¢ãƒ•ã‚£ãƒ«ã‚¿
                    if pd.notna(race_date):
                        horse_past = horse_past[horse_past[self.date_column] < race_date]
                    else:
                        warnings.warn(
                            f"race_dateã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ: {race[self.date_column]}",
                            UserWarning
                        )

                if (
                    len(horse_past) > 0
                    and "baba_index" in horse_past.columns
                    and "finish_position" in horse_past.columns
                ):
                    # é«˜é€Ÿé¦¬å ´ã§ã®æˆç¸¾
                    high_speed_races = horse_past[horse_past["baba_index"] < -1.5]
                    if len(high_speed_races) > 0:
                        high_speed_win_rate = (
                            high_speed_races["finish_position"] == 1
                        ).mean()
                        high_speed_count = int(len(high_speed_races))

                    # æ™‚è¨ˆã‹ã‹ã‚‹é¦¬å ´ã§ã®æˆç¸¾
                    slow_races = horse_past[horse_past["baba_index"] > 1.5]
                    if len(slow_races) > 0:
                        slow_win_rate = (
                            slow_races["finish_position"] == 1
                        ).mean()
                        slow_count = int(len(slow_races))

            features.append(
                {
                    "predicted_baba_index": race.get("predicted_baba_index", 0.0),
                    "normalized_cushion": normalized_cushion,
                    "normalized_moisture": normalized_moisture,
                    "horse_high_speed_win_rate": high_speed_win_rate,
                    "horse_slow_win_rate": slow_win_rate,
                    "horse_high_speed_count": high_speed_count,
                    "horse_slow_count": slow_count,
                    "track_correlation": self.track_statistics.get(track, {}).get(
                        "cushion_correlation", 0.0
                    ),
                }
            )

        return pd.DataFrame(features)


def example_usage():
    """ä½¿ç”¨ä¾‹ï¼ˆv2.0ï¼‰"""

    print("=" * 80)
    print("BabaAdjustmentModel v2.0 - ä½¿ç”¨ä¾‹ï¼ˆå®Ÿé‹ç”¨å®Œæˆç‰ˆï¼‰")
    print("=" * 80)

    np.random.seed(42)
    n = 1000

    calibrated_pred = np.random.beta(2, 8, n)
    actual_win = np.random.binomial(1, 0.1, n)

    features = pd.DataFrame(
        {
            "predicted_baba_index": np.random.normal(0, 1.5, n),
            "normalized_cushion": np.random.normal(0, 1, n),
            "normalized_moisture": np.random.normal(0, 1, n),
            "horse_high_speed_win_rate": np.random.uniform(0, 0.3, n),
            "horse_slow_win_rate": np.random.uniform(0, 0.3, n),
            "horse_high_speed_count": np.random.randint(0, 10, n),
            "horse_slow_count": np.random.randint(0, 10, n),
            "track_correlation": np.random.uniform(-0.5, 0.5, n),
        }
    )

    horse_baba_race_counts = np.random.randint(1, 15, n)

    # ğŸ”¥ v2.0: clip_percentileãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    model = BabaAdjustmentModel(clip_percentile=99.0)
    X_train, y_train, w_train = model.prepare_training_data(
        calibrated_pred=calibrated_pred,
        actual_win=actual_win,
        features=features,
        horse_baba_race_counts=horse_baba_race_counts,
    )

    model.train(X_train, y_train, sample_weight=w_train)

    delta_baba = model.predict_delta(
        features=features,
        horse_baba_race_counts=horse_baba_race_counts,
        apply_shrinkage=True,
    )

    final_prob = model.apply_adjustment(calibrated_pred, delta_baba)

    print("\n=== é¦¬å ´è£œæ­£ã®ä¾‹ ===")
    print("å…ƒã®äºˆæ¸¬ç¢ºç‡:", calibrated_pred[800:805])
    print("delta_baba:", delta_baba[800:805])
    print("è£œæ­£å¾Œç¢ºç‡:", final_prob[800:805])

    print("\n=== ç‰¹å¾´é‡é‡è¦åº¦ ===")
    print(model.get_feature_importance())
    
    print("\n" + "=" * 80)
    print("âœ… v2.0å®Œæˆ - ChatGPTç‰ˆ + Claudeè¿½åŠ ä¿®æ­£å®Œäº†")
    print("=" * 80)


if __name__ == "__main__":
    example_usage()
