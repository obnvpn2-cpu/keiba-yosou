# calibration_usage_examples.py
# Calibration v2 ã®å®Ÿç”¨çš„ãªä½¿ç”¨ä¾‹ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

import numpy as np
import pandas as pd
from typing import Tuple

# å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# from calibration_v2 import CalibrationConfig, ProbabilityCalibrator
# from model_utils import fit_calibrated_base_model


# ============================================================
# Example 1: åŸºæœ¬çš„ãªä½¿ã„æ–¹
# ============================================================

def example_1_basic_usage():
    """æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªä½¿ç”¨ä¾‹"""
    print("=" * 60)
    print("Example 1: åŸºæœ¬çš„ãªä½¿ã„æ–¹")
    print("=" * 60)
    
    from calibration_v2 import ProbabilityCalibrator, CalibrationConfig
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    np.random.seed(42)
    n = 1000
    
    # éä¿¡ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ï¼ˆå®Ÿéš›ã®ç¢ºç‡ã‚ˆã‚Šé«˜ãå‡ºåŠ›ï¼‰
    y_true = np.random.binomial(1, 0.3, n)
    y_pred_overconfident = np.clip(
        np.random.beta(2, 5, n) * 1.5,  # æ„å›³çš„ã«éä¿¡
        0.01, 0.99
    )
    
    # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚¿ã®ä½œæˆã¨å­¦ç¿’
    config = CalibrationConfig(method="platt", n_bins=10)
    calibrator = ProbabilityCalibrator(config)
    calibrator.fit(y_pred_overconfident, y_true)
    
    # äºˆæ¸¬
    y_calibrated = calibrator.predict(y_pred_overconfident)
    
    # è©•ä¾¡
    metrics = calibrator.evaluate(y_pred_overconfident, y_true)
    
    print(f"\nğŸ“Š ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ:")
    print(f"  ECE (Before): {metrics['ece_raw']:.6f}")
    print(f"  ECE (After):  {metrics['ece_calibrated']:.6f}")
    print(f"  æ”¹å–„ç‡: {(1 - metrics['ece_calibrated']/metrics['ece_raw'])*100:.1f}%")
    print(f"\n  Brier (Before): {metrics['brier_raw']:.6f}")
    print(f"  Brier (After):  {metrics['brier_calibrated']:.6f}")
    print(f"  æ”¹å–„ç‡: {(1 - metrics['brier_calibrated']/metrics['brier_raw'])*100:.1f}%")
    print()


# ============================================================
# Example 2: 3ã¤ã®æ‰‹æ³•ã®æ¯”è¼ƒ
# ============================================================

def example_2_compare_methods():
    """Identity / Platt / Isotonic ã®æ¯”è¼ƒ"""
    print("=" * 60)
    print("Example 2: 3ã¤ã®æ‰‹æ³•ã®æ¯”è¼ƒ")
    print("=" * 60)
    
    from calibration_v2 import ProbabilityCalibrator, CalibrationConfig
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    np.random.seed(42)
    n = 2000
    y_true = np.random.binomial(1, 0.4, n)
    y_pred = np.clip(np.random.beta(3, 5, n) * 1.3, 0.01, 0.99)
    
    methods = ["identity", "platt", "isotonic"]
    results = {}
    
    for method in methods:
        config = CalibrationConfig(method=method, n_bins=15)
        calibrator = ProbabilityCalibrator(config)
        calibrator.fit(y_pred, y_true)
        
        metrics = calibrator.evaluate(y_pred, y_true)
        results[method] = metrics
    
    # çµæœè¡¨ç¤º
    print(f"\n{'Method':<12} {'ECE (Raw)':<12} {'ECE (Cal)':<12} {'Improvement':<12}")
    print("-" * 60)
    
    for method, metrics in results.items():
        improvement = (1 - metrics['ece_calibrated']/metrics['ece_raw']) * 100
        print(
            f"{method:<12} "
            f"{metrics['ece_raw']:<12.6f} "
            f"{metrics['ece_calibrated']:<12.6f} "
            f"{improvement:>10.1f}%"
        )
    
    print("\nğŸ’¡ Tip:")
    print("  - Identity: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆä½•ã‚‚ã—ãªã„ï¼‰")
    print("  - Platt: ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼ˆã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§ãƒ•ã‚£ãƒƒãƒˆï¼‰")
    print("  - Isotonic: ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿã ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒˆã—ã‚„ã™ã„ï¼‰")
    print()


# ============================================================
# Example 3: ä¿å­˜ã¨èª­ã¿è¾¼ã¿
# ============================================================

def example_3_save_load():
    """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿"""
    print("=" * 60)
    print("Example 3: ä¿å­˜ã¨èª­ã¿è¾¼ã¿")
    print("=" * 60)
    
    from calibration_v2 import ProbabilityCalibrator, CalibrationConfig
    import os
    import tempfile
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«
    temp_file = os.path.join(tempfile.gettempdir(), "calibrator_temp.pkl")
    
    # å­¦ç¿’
    np.random.seed(42)
    y_true = np.random.binomial(1, 0.3, 1000)
    y_pred = np.clip(np.random.beta(2, 5, 1000), 0.01, 0.99)
    
    calibrator = ProbabilityCalibrator(CalibrationConfig(method="platt"))
    calibrator.fit(y_pred, y_true)
    
    # ä¿å­˜
    calibrator.save(temp_file)
    print(f"âœ… ä¿å­˜å®Œäº†: {temp_file}")
    
    # èª­ã¿è¾¼ã¿
    calibrator2 = ProbabilityCalibrator()
    calibrator2.load(temp_file)
    print(f"âœ… èª­ã¿è¾¼ã¿å®Œäº†")
    
    # åŒã˜çµæœãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
    y_cal_1 = calibrator.predict(y_pred[:10])
    y_cal_2 = calibrator2.predict(y_pred[:10])
    
    assert np.allclose(y_cal_1, y_cal_2), "èª­ã¿è¾¼ã¿å¾Œã®äºˆæ¸¬ãŒä¸€è‡´ã—ã¾ã›ã‚“"
    print("âœ… æ¤œè¨¼OK: ä¿å­˜å‰å¾Œã§åŒã˜äºˆæ¸¬çµæœ")
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    os.remove(temp_file)
    print()


# ============================================================
# Example 4: Reliability Curve ã®å–å¾—
# ============================================================

def example_4_reliability_curve():
    """ãƒªãƒ©ã‚¤ã‚¢ãƒ“ãƒªãƒ†ã‚£ã‚«ãƒ¼ãƒ–ã®å–å¾—ã¨å¯è¦–åŒ–ï¼ˆmatplotlibä½¿ç”¨ï¼‰"""
    print("=" * 60)
    print("Example 4: Reliability Curve")
    print("=" * 60)
    
    from calibration_v2 import ProbabilityCalibrator, CalibrationConfig
    
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        print("âš ï¸  matplotlib ãŒå¿…è¦ã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    np.random.seed(42)
    n = 5000
    y_true = np.random.binomial(1, 0.4, n)
    y_pred = np.clip(np.random.beta(3, 5, n) * 1.2, 0.01, 0.99)
    
    # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‰å¾Œ
    calibrator = ProbabilityCalibrator(CalibrationConfig(method="isotonic"))
    calibrator.fit(y_pred, y_true)
    y_cal = calibrator.predict(y_pred)
    
    # Reliability Curve
    bin_centers_raw, bin_acc_raw, bin_conf_raw = calibrator.get_reliability_curve(
        y_true, y_pred, n_bins=10
    )
    bin_centers_cal, bin_acc_cal, bin_conf_cal = calibrator.get_reliability_curve(
        y_true, y_cal, n_bins=10
    )
    
    # ãƒ—ãƒ­ãƒƒãƒˆ
    plt.figure(figsize=(10, 6))
    
    # Perfect calibration line
    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', alpha=0.5)
    
    # Before calibration
    mask_raw = ~np.isnan(bin_acc_raw)
    plt.plot(
        bin_conf_raw[mask_raw], bin_acc_raw[mask_raw],
        'o-', label='Before Calibration', markersize=8
    )
    
    # After calibration
    mask_cal = ~np.isnan(bin_acc_cal)
    plt.plot(
        bin_conf_cal[mask_cal], bin_acc_cal[mask_cal],
        's-', label='After Calibration', markersize=8
    )
    
    plt.xlabel('Mean Predicted Probability', fontsize=12)
    plt.ylabel('Fraction of Positives', fontsize=12)
    plt.title('Reliability Curve (Calibration Plot)', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    output_path = "/mnt/user-data/outputs/reliability_curve.png"
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"âœ… ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜: {output_path}")
    plt.close()
    print()


# ============================================================
# Example 5: ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã§ã®å®Ÿç”¨ä¾‹
# ============================================================

def example_5_horse_racing_calibration():
    """ç«¶é¦¬ã®å‹ç‡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("=" * 60)
    print("Example 5: ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã§ã®å®Ÿç”¨ä¾‹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰")
    print("=" * 60)
    
    from calibration_v2 import ProbabilityCalibrator, CalibrationConfig
    
    # ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    np.random.seed(42)
    
    # ãƒ¬ãƒ¼ã‚¹æ•°ã¨ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
    n_races = 500
    n_horses_per_race = 16
    n_total = n_races * n_horses_per_race
    
    # å‹ç‡äºˆæ¸¬ï¼ˆå¤šãã®é¦¬ã¯ä½ç¢ºç‡ã€ä¸€éƒ¨ãŒé«˜ç¢ºç‡ï¼‰
    # å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’æ¨¡æ“¬: å¤§åŠãŒ 0.01-0.15ã€ãŸã¾ã« 0.3-0.7
    y_pred = np.concatenate([
        np.random.beta(1, 20, int(n_total * 0.8)),  # å¤§åŠã¯ä½ç¢ºç‡
        np.random.beta(3, 5, int(n_total * 0.2))   # ä¸€éƒ¨ã¯ä¸­ã€œé«˜ç¢ºç‡
    ])
    np.random.shuffle(y_pred)
    y_pred = np.clip(y_pred, 0.001, 0.999)
    
    # å®Ÿéš›ã®å‹ã¡ï¼ˆå„ãƒ¬ãƒ¼ã‚¹ã§1é ­ã®ã¿å‹åˆ©ï¼‰
    y_true = np.zeros(n_total)
    for i in range(n_races):
        race_probs = y_pred[i*n_horses_per_race:(i+1)*n_horses_per_race]
        # ç¢ºç‡ã«æ¯”ä¾‹ã—ã¦å‹è€…ã‚’æ±ºå®š
        winner_idx = np.random.choice(
            n_horses_per_race,
            p=race_probs / race_probs.sum()
        )
        y_true[i*n_horses_per_race + winner_idx] = 1
    
    # Train/Val åˆ†å‰²
    split_idx = int(n_total * 0.7)
    y_pred_train = y_pred[:split_idx]
    y_true_train = y_true[:split_idx]
    y_pred_val = y_pred[split_idx:]
    y_true_val = y_true[split_idx:]
    
    # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆIsotonic ã‚’ä½¿ç”¨ - ç«¶é¦¬ã¯éç·šå½¢æ€§ãŒé«˜ã„ãŸã‚ï¼‰
    config = CalibrationConfig(method="isotonic", n_bins=20, min_samples_bin=30)
    calibrator = ProbabilityCalibrator(config)
    calibrator.fit(y_pred_train, y_true_train)
    
    # è©•ä¾¡
    metrics = calibrator.evaluate(y_pred_val, y_true_val)
    
    print(f"\nğŸ“Š ç«¶é¦¬å‹ç‡äºˆæ¸¬ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ:")
    print(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(y_true_val)} é ­åˆ†")
    print(f"  å®Ÿéš›ã®å‹é¦¬: {int(y_true_val.sum())} é ­")
    print(f"\n  ECE (Before): {metrics['ece_raw']:.6f}")
    print(f"  ECE (After):  {metrics['ece_calibrated']:.6f}")
    print(f"  æ”¹å–„: {(metrics['ece_raw'] - metrics['ece_calibrated']):.6f}")
    print(f"\n  Brier (Before): {metrics['brier_raw']:.6f}")
    print(f"  Brier (After):  {metrics['brier_calibrated']:.6f}")
    
    # ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬
    print(f"\nğŸ“ ã‚µãƒ³ãƒ—ãƒ«äºˆæ¸¬ï¼ˆæœ€åˆã®5é ­ï¼‰:")
    print(f"{'Raw Prob':<12} {'Calibrated':<12} {'Actual':<8}")
    print("-" * 35)
    
    y_cal_val = calibrator.predict(y_pred_val)
    for i in range(min(5, len(y_pred_val))):
        print(
            f"{y_pred_val[i]:<12.4f} "
            f"{y_cal_val[i]:<12.4f} "
            f"{int(y_true_val[i]):<8}"
        )
    
    print("\nğŸ’¡ å®Ÿç”¨ä¸Šã®æ³¨æ„:")
    print("  - ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯ãƒ©ãƒ³ã‚¯é †ä½ã‚’å¤‰ãˆãªã„ï¼ˆAUCã¯ä¸å¤‰ï¼‰")
    print("  - ç¢ºç‡ã®ã€Œçµ¶å¯¾å€¤ã€ã®ä¿¡é ¼æ€§ãŒå‘ä¸Šã™ã‚‹")
    print("  - è³­ã‘æˆ¦ç•¥ï¼ˆã‚ªãƒƒã‚ºã¨ã®æ¯”è¼ƒç­‰ï¼‰ã§ç‰¹ã«é‡è¦")
    print()


# ============================================================
# Example 6: ãƒ“ãƒ³æ•°ã¨ min_samples_bin ã®å½±éŸ¿
# ============================================================

def example_6_hyperparameter_tuning():
    """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿ã‚’ç¢ºèª"""
    print("=" * 60)
    print("Example 6: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿")
    print("=" * 60)
    
    from calibration_v2 import ProbabilityCalibrator, CalibrationConfig
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    np.random.seed(42)
    n = 5000
    y_true = np.random.binomial(1, 0.3, n)
    y_pred = np.clip(np.random.beta(2, 5, n) * 1.4, 0.01, 0.99)
    
    # å„è¨­å®šã§ã®ECEã‚’æ¯”è¼ƒ
    results = []
    
    for n_bins in [5, 10, 15, 20]:
        for min_samples in [10, 20, 50]:
            config = CalibrationConfig(
                method="platt",
                n_bins=n_bins,
                min_samples_bin=min_samples
            )
            calibrator = ProbabilityCalibrator(config)
            calibrator.fit(y_pred, y_true)
            
            metrics = calibrator.evaluate(y_pred, y_true)
            results.append({
                'n_bins': n_bins,
                'min_samples': min_samples,
                'ece': metrics['ece_calibrated']
            })
    
    # çµæœè¡¨ç¤º
    print(f"\n{'n_bins':<10} {'min_samples':<15} {'ECE (Calibrated)':<20}")
    print("-" * 50)
    
    for r in results:
        print(
            f"{r['n_bins']:<10} "
            f"{r['min_samples']:<15} "
            f"{r['ece']:<20.6f}"
        )
    
    print("\nğŸ’¡ Tip:")
    print("  - n_bins ãŒå¤šã„ã»ã©ç´°ã‹ãè©•ä¾¡ã§ãã‚‹ãŒã€å„ãƒ“ãƒ³ã®ã‚µãƒ³ãƒ—ãƒ«ãŒæ¸›ã‚‹")
    print("  - min_samples_bin ã‚’å¤§ããã™ã‚‹ã¨å®‰å®šã™ã‚‹ãŒã€ãƒ“ãƒ³ãŒå°‘ãªããªã‚‹")
    print("  - ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ã¦èª¿æ•´ãŒå¿…è¦")
    print()


# ============================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ============================================================

if __name__ == "__main__":
    print("\n" + "=" * 60)
    print("Calibration v2 ä½¿ç”¨ä¾‹é›†")
    print("=" * 60 + "\n")
    
    # å„ä¾‹ã‚’å®Ÿè¡Œ
    example_1_basic_usage()
    example_2_compare_methods()
    example_3_save_load()
    example_4_reliability_curve()
    example_5_horse_racing_calibration()
    example_6_hyperparameter_tuning()
    
    print("=" * 60)
    print("å…¨ã¦ã®ä¾‹ã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("=" * 60)
